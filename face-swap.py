import argparse
import cv2
import numpy as np
from insightface.app import FaceAnalysis
from gfpgan import GFPGANer
from numpy.linalg import norm
import torch

# ----------------------------
# åˆå§‹åŒ– InsightFace
# ----------------------------
print("Loading InsightFace...")
app = FaceAnalysis(providers=['CPUExecutionProvider'])
app.prepare(ctx_id=-1, det_size=(640, 640))
print("InsightFace loaded.")

# ----------------------------
# Argparse
# ----------------------------
def parse_args():
    parser = argparse.ArgumentParser(description="Face Swap using InsightFace + GFPGAN + Reference Face")
    parser.add_argument('--source', type=str, required=True, help='C:/Users/tony0/Downloads/Homework1/Ma.bmp"')
    parser.add_argument('--input', type=str, required=True, help='C:/Users/tony0/Downloads/Homework1/AI-muscle.mp4')
    parser.add_argument('--output', type=str, required=True, help='C:/Users/tony0/Downloads/Homework1/output.mp4')
    parser.add_argument('--reference', type=str, required=True, help='C:/Users/tony0/Downloads/Homework1/è¢å¹•æ“·å–ç•«é¢ 2025-10-22 094048.png')
    parser.add_argument('--gfpgan', action='store_true', help='Use GFPGAN enhancement')
    parser.add_argument('--threshold', type=float, default=0.3, help='Cosine similarity threshold')
    return parser.parse_args()

# ----------------------------
# è¼‰å…¥å½±åƒ
# ----------------------------
def load_image(path):
    img = cv2.imread(path)
    if img is None:
        raise ValueError(f"Cannot load image from {path}")
    return img

# ----------------------------
# è¨ˆç®— cosine similarity
# ----------------------------
def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (norm(v1) * norm(v2))

# ----------------------------
# åˆå§‹åŒ– GFPGAN
# ----------------------------
def init_gfpgan():
    use_cuda = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 7
    device = "cuda" if use_cuda else "cpu"
    print(f"Initializing GFPGAN on {device}...")

    try:
        gfpgan_model = GFPGANer(
            model_path='experiments/pretrained_models/GFPGANv1.3.pth',
            upscale=1,
            arch='clean',
            device=device
        )
        print(f"GFPGAN loaded on {device}.")
        return gfpgan_model
    except Exception as e:
        print(f"GFPGAN initialization failed: {e}")
        return None

# ----------------------------
# æ›è‡‰å‡½æ•¸
# ----------------------------
def swap_faces(source_img, target_img, ref_embedding, gfpgan=None, alpha=0.7, threshold=0.3):

    source_faces = app.get(source_img)
    target_faces = app.get(target_img)
    if len(source_faces) == 0 or len(target_faces) == 0:
        return target_img

    source_face = source_faces[0]
    source_landmarks = source_face.landmark_2d_106

    output_img = target_img.copy()

    for t_face in target_faces:
        target_embedding = t_face.embedding
        sim = cosine_similarity(ref_embedding, target_embedding)
        if sim < threshold:
            continue

        target_landmarks = t_face.landmark_2d_106

        # ğŸŸ¡ 1ï¸âƒ£ é¡é ­è£œé»
        extra_points = []
        top_y = np.min(target_landmarks[:, 1])
        top_x = np.mean(target_landmarks[:, 0])
        extra_points.append([top_x, top_y - 40])
        extra_points.append([top_x - 30, top_y - 30])
        extra_points.append([top_x + 30, top_y - 30])

        # åˆä½µåŸè‡‰èˆ‡é¡é ­é»
        all_landmarks = np.vstack([target_landmarks, np.array(extra_points)])

        # ğŸŸ¢ 2ï¸âƒ£ æ”¾å¤§æ•´é«”é®ç½©ç¯„åœ
        center = np.mean(all_landmarks, axis=0)
        scale_x = 1.05
        scale_y = 1.15
        expanded_landmarks = (all_landmarks - center) * [scale_x, scale_y] + center

        # ğŸŸ£ 3ï¸âƒ£ å»ºç«‹æœ€çµ‚é®ç½©
        mask = np.zeros(target_img.shape[:2], dtype=np.uint8)
        cv2.fillConvexPoly(mask,
                        cv2.convexHull(expanded_landmarks.astype(np.int32)),
                        255)

        # ğŸŸ  4ï¸âƒ£ å°é½Šè‡‰éƒ¨
        hull_index = cv2.convexHull(target_landmarks.astype(np.int32), returnPoints=False)
        M, _ = cv2.estimateAffinePartial2D(
            source_landmarks[hull_index[:, 0]],
            target_landmarks[hull_index[:, 0]]
        )
        warped_source = cv2.warpAffine(source_img, M, (target_img.shape[1], target_img.shape[0]))

        mask = np.zeros(target_img.shape[:2], dtype=np.uint8)
        cv2.fillConvexPoly(mask, cv2.convexHull(target_landmarks.astype(np.int32)), 255)


        if gfpgan is not None:
            try:
                restored_faces, restored_img, _ = gfpgan.enhance(
                    warped_source,
                    has_aligned=False,
                    only_center_face=False,
                    paste_back=True  # å°‡ä¿®å¾©çš„äººè‡‰è²¼å›åŸåœ–
                )
                if restored_img is not None and isinstance(restored_img, np.ndarray) and restored_img.size > 0:
                    warped_source = restored_img
            except Exception as e:
                print(f"GFPGAN enhance failed, fallback to original warp: {e}")
                # å¤±æ•—å°±ä½¿ç”¨åŸæœ¬ warpï¼Œä¸è¿”å› None
                pass

        target_region = cv2.bitwise_and(output_img, output_img, mask=cv2.bitwise_not(mask))
        source_region = cv2.bitwise_and(warped_source, warped_source, mask=mask)
        blended = cv2.addWeighted(target_region, 1-alpha, source_region, alpha, 0)
        output_img[np.where(mask==255)] = blended[np.where(mask==255)]

    return output_img


# ----------------------------
# å½±ç‰‡è™•ç†
# ----------------------------
def process_video(source_img, input_path, output_path, ref_embedding, gfpgan=None, threshold=0.3):
    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        raise ValueError(f"Cannot open video {input_path}")

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    frame_idx = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        swapped = swap_faces(source_img, frame, ref_embedding, gfpgan=gfpgan, threshold=threshold)
        if swapped is None:
            cap.release()
            out.release()
            print("\nGFPGAN failed during video processing. Re-running without GFPGAN...")
            return False

        out.write(swapped)
        frame_idx += 1

        # é¡¯ç¤ºç™¾åˆ†æ¯”é€²åº¦æ¢
        progress = (frame_idx / total_frames) * 100
        bar_length = 30  # é€²åº¦æ¢é•·åº¦
        filled_length = int(bar_length * frame_idx // total_frames)
        bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
        print(f"\rProgress: |{bar}| {progress:6.2f}% ({frame_idx}/{total_frames})", end='')

    cap.release()
    out.release()
    print("\nVideo processing finished.")
    return True


# ----------------------------
# ä¸»ç¨‹å¼
# ----------------------------
def main():
    args = parse_args()
    source_img = load_image(args.source)
    reference_img = load_image(args.reference)

    ref_faces = app.get(reference_img)
    if len(ref_faces) == 0:
        raise ValueError("Reference face not detected!")
    ref_embedding = ref_faces[0].embedding

    gfpgan_model = None
    if args.gfpgan:
        gfpgan_model = init_gfpgan()

    success = process_video(source_img, args.input, args.output, ref_embedding, gfpgan=gfpgan_model, threshold=args.threshold)

    # å¦‚æœ GFPGAN æ•´æ®µå½±ç‰‡å¤±æ•—ï¼Œé‡æ–°è·‘ä¸€æ¬¡ CPU-only
    if not success:
        print("Re-running video without GFPGAN...")
        process_video(source_img, args.input, args.output, ref_embedding, gfpgan=None, threshold=args.threshold)

if __name__ == "__main__":
    main()



    
# ã€ç›®æ¨™ã€‘  
# ã€Objectivesã€‘
# Develop a Python program that performs face swap technique on a video using the
# InsightFace library. The user should be able to specify the source face image, input video,
# output video, and implement face swap technique with pipeline processes.

# ã€Learning Goalsã€‘
# âš« Use Argparse to create a flexible command-line Python program.
# âš« Apply InsightFace for face detection and face swap.
# âš« Process video with OpenCV and generate output.
# âš« Understand and implement the AI multimedia pipeline using Python programming.

# ã€Python ç¨‹å¼è¨­è¨ˆã€‘
# Design a Python program to perform face swapping pipeline
# (Pipeline æ˜¯æŒ‡å¾è¼¸å…¥åˆ°è¼¸å‡ºã€Œä¸€æ¢é¾ã€å®Œæˆ)ï¼š
# python face_swap.py --source source.jpg --input input.mp4 --output output.mp4
# ä½ å¯ä»¥è‡ªå®šç¾©å…¶ä»–åƒæ•¸ (Arguments)ï¼Œä¾‹å¦‚ï¼š--helpã€--reference ç­‰ã€‚
# ã€Requirementsã€‘
# è«‹åŒå­¸ä»¥çµ„ç‚ºå–®ä½ï¼Œå®Œæˆä¸‹åˆ—äº‹é …ï¼š
# âš« ä½¿ç”¨ Argparse è™•ç†åƒæ•¸ã€‚
# âš« è¼‰å…¥è‡‰éƒ¨åµæ¸¬èˆ‡æ›è‡‰æ¨¡å‹ (å‹™å¿…ä¸€æ¬¡æ€§è¼‰å…¥)ã€‚
# âš« ä»¥é€å¹€ (frame-by-frame) æ–¹å¼è™•ç†æ•¸ä½è¦–è¨Šã€‚
# (åŸå‰‡ä¸Šï¼Œå»ºè­°å…ˆä½¿ç”¨å–®å¼µå½±åƒé€²è¡Œç¨‹å¼è¨­è¨ˆèˆ‡æ¸¬è©¦ï¼Œå½±åƒç¯„ä¾‹å¦‚ä¸‹åœ–)ã€‚
# âš« åµæ¸¬æ¯ä¸€å¹€ä¸­çš„è‡‰éƒ¨å€åŸŸã€‚
# âš« ä½¿ç”¨ä¾†æºäººè‡‰å½±åƒé€²è¡Œæ›è‡‰ (è‹¥è‡‰éƒ¨å€åŸŸç•¥è¶…å‡ºç•«é¢ç¯„åœï¼Œä»ç„¶é ˆé€²è¡Œæ›è‡‰)ã€‚
# âš« å°ç½®æ›å¾Œçš„è‡‰éƒ¨å€åŸŸä½¿ç”¨ Real-ESRGAN æˆ– GFPGAN æŠ€è¡“å¢å¼·ç´°ç¯€ï¼Œä¸¦è²¼å›ç›®æ¨™å½±
# åƒã€‚å»ºè­°åœ¨æ›è‡‰éç¨‹ï¼Œé¡¯ç¤ºå½±åƒçµæœã€‚
# âš« å°‡æ›è‡‰å¾Œçš„å½±ç‰‡å„²å­˜è‡³--output æŒ‡å®šçš„è·¯å¾‘ã€‚
# âš« è‹¥æœ‰éŸ³è¨Šï¼Œå‰‡ä½¿ç”¨ FFmpeg é‡æ–°åˆæˆè¦–è¨Šèˆ‡éŸ³è¨Š (ä½¿ç”¨ Subprocess)ï¼Œä¸¦è¼¸å‡ºè¦–è¨Šæª”
# æ¡ˆçµæœã€‚

# ã€Improvementsã€‘
# è«‹åŒå­¸ä»¥çµ„ç‚ºå–®ä½ï¼Œé€²ä¸€æ­¥å¢å¼·ä¸‹åˆ—ç¨‹å¼åŠŸèƒ½ï¼š
# âš« é¦–å…ˆï¼Œæ–¼å½±ç‰‡ä¸­æ“·å–ä¸»è§’çš„æ­£é¢è‡‰éƒ¨å½±åƒï¼Œä½œç‚ºåƒè€ƒè‡‰éƒ¨å½±åƒ (Reference Face
# Image)ã€‚
# âš« ä½¿ç”¨ InsightFace çš„äººè‡‰ embedding vector (åµŒå…¥ç‰¹å¾µå‘é‡) æ¯”å°ä¾†è¾¨è­˜ã€Œä¸»è§’ã€ã€‚
# âš« åªæœ‰ç•¶å½±ç‰‡ä¸­åµæ¸¬åˆ°çš„è‡‰èˆ‡ --reference ä¸»è§’è‡‰éå¸¸ç›¸ä¼¼ (ä¾‹å¦‚ï¼šcosine similarity
# ï‚³ 0.3) æ™‚ï¼Œæ‰é€²è¡Œæ›è‡‰ï¼Œä»¥ç¢ºä¿ä¸æœƒèª¤å¥—å…¶ä»–äººè‡‰ã€‚
# âš« è²¼å›è‡‰éƒ¨å½±åƒå€åŸŸæ™‚ï¼Œå¯å˜—è©¦æ¡ç”¨ Alpha Blending æŠ€è¡“é€²è¡Œæ‹¼è²¼ï¼Œä½¿å¾—è‡‰éƒ¨å€åŸŸé‚Š
# ç·£å€åŸŸè¼ƒç‚ºè²¼åˆã€‚
# âš« ä½¿ç”¨ FFmpeg æˆ–å¨åŠ›å°æ¼” (ä¸è¦ä½¿ç”¨è©¦ç”¨ç‰ˆï¼Œæœƒæœ‰æµ®æ°´å°)ï¼Œé€²è¡Œæœ€å¾Œçš„èª¿æ•´


